{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c79cf97",
   "metadata": {},
   "source": [
    "# 5.3 - Boosting\n",
    "\n",
    "O termo 'Boosting' refere-se a uma família de algoritmos que converte alunos fracos em alunos fortes. Boosting é um método de conjunto para melhorar as previsões do modelo de qualquer algoritmo de aprendizagem. A ideia do boost é treinar sequencialmente os alunos fracos, cada um tentando corrigir seu antecessor.\n",
    "\n",
    "![fig_1](https://i.ibb.co/qCtThMm/random-forest-drawio-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8e8a0",
   "metadata": {},
   "source": [
    "## Tipos de Boosting\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "Adaboost combina vários alunos fracos em um único aluno forte. Os alunos fracos no AdaBoost são árvores de decisão com uma única divisão, chamadas de tocos de decisão. Quando o AdaBoost cria seu primeiro toco de decisão, todas as observações são ponderadas igualmente. Para corrigir o erro anterior, as observações que foram classificadas incorretamente agora têm mais peso do que as observações que foram classificadas corretamente. Os algoritmos AdaBoost podem ser usados para problemas de classificação e regressão.\n",
    "\n",
    "![fig_2](https://i.ibb.co/0J5D6fH/boosting-1-drawio.png)\n",
    "\n",
    "Como podemos ver acima, o primeiro coto de decisão (D1) é feito separando a região (+) azul da região (-) vermelha. Notamos que D1 tem três classificados incorretamente (+) na região vermelha. A classificação incorreta (+) agora terá mais peso do que as outras observações e será fornecida ao segundo aluno. O modelo continuará e ajustará o erro enfrentado pelo modelo anterior até que o preditor mais preciso seja construído."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9dee0c",
   "metadata": {},
   "source": [
    "#### Em Python\n",
    "\n",
    "* Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c141192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18360e06",
   "metadata": {},
   "source": [
    "* Importando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5149e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404f3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06455b",
   "metadata": {},
   "source": [
    "* Construindo o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2838586",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=50,\n",
    "                         learning_rate=1)\n",
    "model = abc.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12916d65",
   "metadata": {},
   "source": [
    "Os parâmetros mais importantes são:\n",
    "* <code>base_estimator</code>: É um aluno fraco usado para treinar o modelo. Ele usa o DecisionTreeClassifier como aluno fraco padrão para fins de treinamento. Você também pode especificar diferentes algoritmos de aprendizado de máquina;\n",
    "* <code>n_estimators</code>: Número de alunos fracos para treinar iterativamente;\n",
    "* <code>learning_rate</code>: \n",
    "Contribui para o peso dos alunos fracos. Ele usa 1 como valor padrão\n",
    "\n",
    "Avaliando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053e4e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:{:.2f}\".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1270be0",
   "metadata": {},
   "source": [
    "#### Vantagens:\n",
    "\n",
    "AdaBoost é fácil de implementar. Ele corrige iterativamente os erros do classificador fraco e melhora a precisão combinando alunos fracos. Você pode usar muitos classificadores básicos com AdaBoost. AdaBoost não é propenso a overfitting. Isso pode ser descoberto por meio dos resultados do experimento, mas não há nenhuma razão concreta disponível.\n",
    "\n",
    "#### Desvantagens:\n",
    "\n",
    "AdaBoost é sensível a dados de ruído. É altamente afetado por outliers porque tenta ajustar cada ponto perfeitamente. AdaBoost é mais lento em comparação com XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be310a",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Assim como o AdaBoost, o Gradient Boosting funciona adicionando preditores sequencialmente a um conjunto, cada um corrigindo seu predecessor. No entanto, em vez de alterar os pesos para cada observação classificada incorreta em cada iteração como AdaBoost, o método Gradient Boosting tenta ajustar o novo preditor aos erros residuais feitos pelo preditor anterior.\n",
    "\n",
    "![fig_3](https://miro.medium.com/max/700/1*FQulhlORGZcoUByMORaLAA.png)\n",
    "\n",
    "\n",
    "GBM usa Gradient Descent para encontrar as deficiências nas previsões do aluno anterior. O algoritmo GBM pode ser fornecido seguindo as etapas:\n",
    "* Ajustar um modelo aos dados, F1 (x) = y\n",
    "* Ajuste um modelo aos resíduos, h1 (x) = y − F1 (x)\n",
    "* Crie um novo modelo, F2 (x) = F1 (x) + h1 (x)\n",
    "* Ao combinar aluno fraco após aluno fraco, nosso modelo final é capaz de dar conta de grande parte do erro do modelo original e reduz esse erro com o tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9506b9",
   "metadata": {},
   "source": [
    "#### Em Python\n",
    "\n",
    "Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41028fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81964c7",
   "metadata": {},
   "source": [
    "Usando o mesmo conjunto de dados anterior e aplicando o método de Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b37f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbf = GradientBoostingClassifier() \n",
    "model_gbc = gbf.fit(X_train, y_train)\n",
    "y_pred_gbc = model_gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1af90",
   "metadata": {},
   "source": [
    "Alguns dos parâmetros esperados por este algoritmo incluem:\n",
    "\n",
    "* <code>perda</code>: definindo a função de perda a ser otimizada;\n",
    "* <code>learning_rate</code>: que determina a contribuição de cada árvore;\n",
    "* <code>n_estimatorst</code>: dita o número de árvores de decisão;\n",
    "* <code>max_depth</code>: é a profundidade máxima de cada estimador;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fa6a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:{:.2f}\".format(metrics.accuracy_score(y_test, y_pred_gbc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15facc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
