{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd101b38-fe2b-4591-acd8-6889e58f916b",
   "metadata": {},
   "source": [
    "# Redes Neurais\n",
    "\n",
    "Redes Neurais são uma classe de modelos dentro da literatura geral de aprendizado de máquina. As redes neurais são um conjunto específico de algoritmos que revolucionaram o aprendizado de máquina. Eles são inspirados por redes neurais biológicas e as chamadas redes neurais profundas têm provado funcionar muito bem. Redes Neurais são em si mesmas aproximações de funções gerais, e é por isso que podem ser aplicadas a quase qualquer problema de aprendizado de máquina sobre o aprendizado de um mapeamento complexo do espaço de entrada para o espaço de saída.\n",
    "\n",
    "## Alguns modelos de Redes Neurais e suas aplicações\n",
    "\n",
    "### 01 - **Perceptrons**\n",
    "\n",
    "Considerada a primeira geração de redes neurais, os perceptrons são simplesmente modelos computacionais de um único neurônio. Perceptron foi originalmente cunhado por Frank Rosenblatt (“O perceptron: um modelo probabilístico para armazenamento e organização de informações no cérebro”). Também chamado de rede neural feed-forward, um perceptron alimenta informações da frente para trás. O treinamento de perceptrons geralmente requer retropropagação, dando à rede conjuntos de dados emparelhados de entradas e saídas. As entradas são enviadas para o neurônio, processadas e resultam em uma saída. O erro que é propagado de volta geralmente é a diferença entre os dados de entrada e de saída. Se a rede tiver neurônios ocultos suficientes, ela sempre pode modelar a relação entre a entrada e a saída. Praticamente, seu uso é muito mais limitado, mas eles são popularmente combinados com outras redes para formar novas redes.\n",
    "\n",
    "![fig_1](https://process.filestackapi.com/cache=expiry:max/SJoLaAi2RBCHbkDlrwaM)\n",
    "\n",
    "Se você escolher os recursos manualmente e tiver o suficiente, poderá fazer quase tudo. Para vetores de entrada binários, podemos ter uma unidade de característica separada para cada um dos muitos vetores binários exponencialmente e podemos fazer qualquer discriminação possível para vetores de entrada binários. No entanto, perceptrons têm limitações: uma vez que as características codificadas à mão tenham sido determinadas, existem limitações muito fortes sobre o que um perceptron pode aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5f2ea-9db5-49a5-8166-1b0610625aed",
   "metadata": {},
   "source": [
    "### 02 - **Redes Neurais Convolucionais**\n",
    "\n",
    "Em 1998, Yann LeCun e seus colaboradores desenvolveram um reconhecedor realmente bom para dígitos manuscritos chamado LeNet. Ele usou retropropagação em uma rede feedforward com muitas camadas ocultas, muitos mapas de unidades replicadas em cada camada, pool de saída de unidades replicadas próximas, uma ampla rede que pode lidar com vários personagens ao mesmo tempo, mesmo se eles se sobrepõem, e um sistema inteligente forma de treinar um sistema completo, não apenas um reconhecedor. Posteriormente, foi formalizado com o nome **redes neurais convolucionais (CNNs)**.\n",
    "\n",
    "![fig_2](https://process.filestackapi.com/cache=expiry:max/8aKhsbgSEGP9nbOUqntB)\n",
    "\n",
    "\n",
    "As redes neurais convolucionais são bastante diferentes da maioria das outras redes. Eles são usados principalmente para processamento de imagem, mas também podem ser usados ​​para outros tipos de entrada, como áudio. Um caso de uso típico para CNNs é quando você alimenta as imagens da rede e ele classifica os dados. CNNs tendem a começar com um “scanner” de entrada, que não se destina a analisar todos os dados de treinamento de uma vez. Por exemplo, para inserir uma imagem de 100 x 100 pixels, você não gostaria de uma camada com 10.000 nós. Em vez disso, você cria uma camada de entrada de digitalização de, digamos, 10 x 10, e alimenta os primeiros 10 x 10 pixels da imagem. Depois de passar essa entrada, você alimenta os próximos 10 x 10 pixels movendo o scanner um pixel para a direita.\n",
    "\n",
    "![fig_3](https://process.filestackapi.com/cache=expiry:max/xGwKNo2Q3C1RMlnxu3h1)\n",
    "\n",
    "Esses dados de entrada são então alimentados por camadas convolucionais em vez de camadas normais, onde nem todos os nós estão conectados. Cada nó se preocupa apenas com células vizinhas próximas. Essas camadas convolucionais também tendem a encolher à medida que se tornam mais profundas, principalmente por fatores facilmente divisíveis da entrada. Ao lado dessas camadas convolucionais, eles também costumam apresentar camadas de pool. O pooling é uma maneira de filtrar os detalhes: uma técnica de pooling comumente encontrada é o pooling máximo, em que pegamos, digamos, 2 x 2 pixels e passamos o pixel com a maior quantidade de vermelho."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d1010-5d3a-4493-8cd0-e7228f0ed510",
   "metadata": {},
   "source": [
    "### 03 - **Redes Neurais Recorrentes**\n",
    "\n",
    "Para entender os RNNs, precisamos ter uma breve visão geral da modelagem de sequência. Ao aplicar o aprendizado de máquina a sequências, geralmente queremos transformar uma sequência de entrada em uma sequência de saída que vive em um domínio diferente. Por exemplo, transforme uma sequência de pressões sonoras em uma sequência de identidades de palavras. Quando não há uma sequência de destino separada, podemos obter um sinal de ensino tentando prever o próximo termo na sequência de entrada. A sequência de saída de destino é a sequência de entrada com um avanço de uma etapa. Isso parece muito mais natural do que tentar prever um pixel em uma imagem a partir dos outros pixels, ou um trecho de uma imagem a partir do restante da imagem. Prever o próximo termo em uma sequência confunde a distinção entre aprendizado supervisionado e não supervisionado. Ele usa métodos projetados para aprendizagem supervisionada, mas não requer um sinal de ensino separado.\n",
    "\n",
    "![fig_4](https://process.filestackapi.com/cache=expiry:max/7nQ8vKKeT56w3mFAg64U)\n",
    "\n",
    "Modelos sem memória são a abordagem padrão para essa tarefa. Em particular, os modelos autorregressivos podem prever o próximo termo em uma sequência a partir de um número fixo de termos anteriores usando \"toques de atraso\". Redes neurais feed-forward são modelos autoregressivos generalizados que usam uma ou mais camadas de unidades ocultas não lineares. No entanto, se dermos ao nosso modelo gerador algum estado oculto, e se dermos a esse estado oculto sua própria dinâmica interna, obteremos um tipo de modelo muito mais interessante, que pode armazenar informações em seu estado oculto por um longo tempo. Se a dinâmica e a maneira como ela gera saídas de seu estado oculto forem ruidosas, nunca saberemos seu estado oculto exato. O melhor que podemos fazer é inferir uma distribuição de probabilidade no espaço de vetores de estado ocultos. Essa inferência é tratável apenas para dois tipos de modelos de estado oculto.\n",
    "\n",
    "Originalmente introduzidas em “Finding structure in time” de Jeffrey Elman (1990), as redes neurais recorrentes (RNNs) são basicamente perceptrons. No entanto, ao contrário dos perceptrons, que não têm estado, eles têm conexões entre passes, conexões através do tempo. Os RNNs são muito poderosos porque combinam duas propriedades: 1) um estado oculto distribuído que permite armazenar muitas informações sobre o passado de forma eficiente e 2) dinâmica não linear que permite atualizar seu estado oculto de maneiras complicadas. Com neurônios e tempo suficientes, os RNNs podem computar qualquer coisa que seu computador possa computar. Então, que tipo de comportamento os RNNs podem exibir? Eles podem oscilar, estabelecer-se para apontar atratores e se comportar de forma caótica. Eles podem aprender a implementar muitos pequenos programas, cada um deles capturando um fragmento de conhecimento e sendo executados em paralelo, interagindo para produzir efeitos muito complicados.\n",
    "\n",
    "![fig_5](https://process.filestackapi.com/cache=expiry:max/AFcrkNbxSTqlT3Rx6wLI)\n",
    "\n",
    "Um grande problema com os RNNs é o problema do gradiente de desaparecimento (ou explosão), onde, dependendo das funções de ativação utilizadas, as informações se perdem rapidamente com o tempo. Intuitivamente, isso não seria um grande problema, porque esses são apenas pesos e não estados de neurônios, mas os pesos ao longo do tempo são, na verdade, onde as informações do passado são armazenadas. Se o peso atingir um valor de 0 ou 1.000.000, o estado anterior não será muito informativo. Os RNNs podem, em princípio, ser usados em muitos campos, pois a maioria das formas de dados que não têm realmente uma linha do tempo (sem áudio ou vídeo) podem ser representados como uma sequência. Uma imagem ou sequência de texto pode ser alimentada com um pixel ou caractere por vez, então pesos dependentes de tempo são usados para o que veio antes na sequência, não realmente o que aconteceu x segundos antes. Em geral, as redes recorrentes são uma boa escolha para avançar ou completar informações, como o preenchimento automático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dae31ca-743b-41d6-bf1d-1b9a13884b01",
   "metadata": {},
   "source": [
    "### 04 - **Hopfield Network**\n",
    "\n",
    "Redes recorrentes de unidades não lineares são geralmente muito difíceis de analisar. Eles podem se comportar de muitas maneiras diferentes: estabelecer-se em um estado estável, oscilar ou seguir trajetórias caóticas que não podem ser previstas em um futuro distante. Para resolver este problema, John Hopfield introduziu a Hopfield Net em seu trabalho de 1982 “Redes neurais e sistemas físicos com habilidades computacionais coletivas emergentes”. Uma rede Hopfield (HN) é uma rede em que cada neurônio está conectado a todos os outros neurônios. É um prato de espaguete completamente emaranhado, pois até mesmo todos os nós funcionam como tudo. Cada nó é inserido antes do treinamento, em seguida, oculto durante o treinamento e saída posteriormente. As redes são treinadas definindo o valor dos neurônios para o padrão desejado, após o qual os pesos podem ser calculados. Os pesos não mudam depois disso. Uma vez treinada para um ou mais padrões, a rede sempre convergirá para um dos padrões aprendidos porque a rede só é estável nesses estados.\n",
    "\n",
    "![fig_7](https://process.filestackapi.com/cache=expiry:max/NFSF09VR8mvnRKsZyqQ4)\n",
    "\n",
    "Existe outra função computacional para as redes de Hopfield. Em vez de usar a rede para armazenar memórias, nós a usamos para construir interpretações de dados sensoriais. A entrada é representada pelas unidades visíveis, os estados das unidades ocultas e a maldade da interpretação é representada pela energia.\n",
    "\n",
    "Infelizmente, as pessoas mostraram que uma rede Hopfield é muito limitada em sua capacidade. Uma rede de Hopfield de N unidades só pode memorizar padrões de 0,15N por causa dos chamados mínimos espúrios em sua função de energia. A ideia é que, uma vez que a função de energia é contínua no espaço de seus pesos, se dois mínimos locais estiverem muito próximos, eles podem \"cair\" um no outro para criar um único mínimo local que não corresponde a nenhuma amostra de treinamento, enquanto esquecendo-se das duas amostras que deve memorizar. Este fenômeno limita significativamente o número de amostras que uma rede Hopfield pode aprender."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
