{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f971d371-9232-4c47-8fc9-b834a9c95467",
   "metadata": {},
   "source": [
    "# 3 - Cross Validation\n",
    "\n",
    "    O primeiro passo no desenvolvimento de modelo de machine learning é o treino e a validação. Para podermos treinar e validar uma modelo, precisamos primeiramente particionar os dados, o que envolve escolher qual a porcentagem de dados usados para treinamento, validação e holdout sets. Abaixo temos um emxplo mostrando um dataset com dados para treino de 64%, 16% dados de validação e 20% de dados holdout.\n",
    "\n",
    "<img src = \"https://www.datarobot.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-22-at-1.22.04-PM.png\" alt=\"fig1\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3263c-b226-42df-b272-a46f46bf9518",
   "metadata": {},
   "source": [
    "* Conjunto de Treino:\n",
    "    * É subdivisão de um dataset do qual o **Algoritmo de Machine Learning** descorbre, \"aprende\", relações entre features e a variável alvo. No machine learning supervisionado, os dados de treino são rotulados com resultados conhecidos.\n",
    "    \n",
    "    \n",
    "Agora vamos identificar os métodos que podemos usar na partição dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e98b0-86b4-4601-ab5c-b35b51a01f8b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.1 - Hold-out\n",
    "\n",
    "#### 3.1.2 - Definição:\n",
    "\n",
    "* É o processo de dividir os dados diferentes divisões e usar uma delas para treinar o modelo e o outro para validar o treino. Sendo usado tanto na **avaliação do modelo** quanto na **seleção do modelo**.\n",
    "\n",
    "<img src = \"https://i.ibb.co/bQmhd81/fig-1-1.png\" alt=\"fig1\" style=\"width:400px;\"/>\n",
    "\n",
    "* No diagrama acima, podemos notar que o conjunto de dados está dividido em duas partes. Uma dessas divisões é separada ou hold-out para treinar o modelo. A outra é separada para testar ou avaliar o modelo. Vamos verificar o processo de usar o método hold-out para avaliação de modelo:\n",
    "    * Dividir o conjunto de dados em duas partes (normalmente dividido em 70%-30%, entretanto, essa porcentagem pode variar dependendo do caso);\n",
    "    * Treinar o modelo no conjunto de dados para treino, enquanto o modelo é treinado, algum conjunto fixo de hiperparâmetros é selecionado;\n",
    "    * Testar ou avaliar o modelo no hold-out do conjunto de dados teste.\n",
    "    * Treinar o modelo final em todo o conjunto de dados para conseguir um modelo que consiga generalizar melhor o conjunto de dados ainda não visto.\n",
    "    \n",
    "* Método Hold-out para Seleção de Modelo\n",
    "    * Ainda podemos usá-lo para seleção de modelo / hyperparameter tunning. No método hold-out para seleção de modelo, o conjunto de dados é dividido em três conjuntos diferentes:\n",
    "        * Treino;\n",
    "        * Validação;\n",
    "        * Conjutno Teste; \n",
    "        \n",
    "<img src = \"https://i.ibb.co/mSJcbB3/fig-1-4.png\" alt=\"fig2\" style=\"width:400px;\"/>\n",
    "\n",
    "\n",
    "* Vamos verificar agora o processo que representa a seleção do método hold-out:\n",
    "    * 1 - Dividir o conjunto de dados em três partes (conjunto de treino, conjunto de validação e conjunto de teste);\n",
    "    * 2 - Treinar modelos diferentes usando algoritmos de machine learning. Por exemplo, treinar um modelo de classificação usando logistic regression, random forest, XGBoost.\n",
    "    * 3 - Para os modelos treinados com algoritmos diferentes, ajustar o hyperparameter e conseguir modelos diferentes. Para cada algoritmo mencionado no passo 2, mude o as configurações do hyperparameter e consiga multiplos modelos;\n",
    "    * 4 - Teste o desempenho de cada um dos modelos no conjunto validação;\n",
    "    * 5 - Selecione the melhor modelo dentre todos os testados no conjunto de validação. O melhor modelo terá a melhor configuração de hyperparameter para um algoritimo específico;\n",
    "    * 6 - Teste o desempenho do melhor modelo no conjunto teste;\n",
    "* Vamos verificar o definido acima no diagrama abaixo:    \n",
    "\n",
    "<img src = \"https://i.ibb.co/Z1XGKw9/fig-1-5.png\" alt=\"fig3\" style=\"width:400px;\"/>\n",
    "\n",
    "### 3.1.2 - Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f6d2f5f-36c7-4e3f-89f6-622dcd82f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- importando bibliotecas ---\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# --- carregando iris dataset ---\n",
    "X = sns.load_dataset('iris')[20:]\n",
    "y = X.pop('species')\n",
    "\n",
    "# --- treinando o modelo ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1712e-6489-4bd3-a87a-52e5973799de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.2 - Leave p-out cross-validation\n",
    "\n",
    "### Definição: \n",
    "\n",
    "* Leave p-out cross-validation (LpOCV) é um método de cross-validation exautivo, que envolve usar observação-p como validação de dados, e o restante dos dados para treinar o modelo. Isso é repitido de todas as maneiras para cortar a amostra original em um conjunto de validação de p observações e um conjunto de treino.\n",
    "\n",
    "* Uma variante de LpOCV com p = 2 conhecido como leave-pair-out cross-validation tem sido recomendado como um método quase imparcial para testar a área abaixo da curva ROC de uma classificação binária.\n",
    "\n",
    "* A imagem abaixo ilustra em caso de p = 3.\n",
    "\n",
    "<img src = \"https://i.ibb.co/0Xw0Gfq/fig-1-6.png\" alt=\"fig5\" style=\"width:400px;\"/>\n",
    "\n",
    "* Vantagens:\n",
    "    * 1 - Simples, fácil de entender e implementar;\n",
    "* Desvantagens:\n",
    "    * 1 - Não é adequado para um conjunto de dados desiquilibrado;\n",
    "    * 2 - Uma grande quantia de dados é isolado do modelo de treino;\n",
    "\n",
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ecd874-afcc-4a7c-b66b-4b396bfdc1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeavePOut(p=2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeavePOut\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "lpo = LeavePOut(2)\n",
    "lpo.get_n_splits(X)\n",
    "print(lpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec8f6693-06f2-4aca-90d4-8ae3c07e57d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [1 3] TEST: [0 2]\n",
      "TRAIN: [1 2] TEST: [0 3]\n",
      "TRAIN: [0 3] TEST: [1 2]\n",
      "TRAIN: [0 2] TEST: [1 3]\n",
      "TRAIN: [0 1] TEST: [2 3]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in lpo.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be864b5-0ef8-4a8c-b44c-2dce1819344c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.3 - Leave-one-out\n",
    "\n",
    "### 3.3.1 - Definição:\n",
    "\n",
    "* Leave-one-out cross-validation (LOOCV) é um método de cross-validation exaustivo. É uma categoria de LpOCV com um caso de p = 1 (logo acima temos um LpOCV com um caso de p = 2).\n",
    "\n",
    "<img src = \"https://i.ibb.co/WFxmMwK/fig-1-7.png\" alt=\"fig6\" style=\"width:400px;\"/>\n",
    "\n",
    "* Vantagens:\n",
    "    * 1 - Simples, fácil de entender e implementar\n",
    "* Desvantagens:\n",
    "    * 1 - O modelo pode ser levado a um baixo viés;\n",
    "    * 2 - O tempo de processamento é alto;\n",
    "    \n",
    "### 3.3.2 - Python    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd08b0de-4662-4d01-8bd7-7227d0ab8eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeaveOneOut()\n",
      "TRAIN: [1] TEST: [0]\n",
      "[[3 4]] [[1 2]] [2] [1]\n",
      "TRAIN: [0] TEST: [1]\n",
      "[[1 2]] [[3 4]] [1] [2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([1, 2])\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "print(loo)\n",
    "LeaveOneOut()\n",
    "for train_index, test_index in loo.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c94ba-45ac-4502-8c8a-eeef4e12382e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.3 - K-Fold\n",
    "\n",
    "### Definição:\n",
    "* Em k-fold cross-validation, o conjunto de dados original é igualmente particionado em $k$ subpartes ou folds. Fora dos k-folds ou grupos, para cada iteração, um grupo é selecionado como dado de confirmação, e os grupos restantes ($k-1$) são selecionados como dados de treino. \n",
    "\n",
    "![gif.1](https://miro.medium.com/max/700/1*2rRcNnIokzJU_-NXm29IMA.gif)\n",
    "\n",
    "* O processo se repete por $k$ vezes até cada grupo ser tratado como validação e permanecendo como dados de treino.\n",
    "\n",
    "\n",
    "<img src = \"https://i.ibb.co/MswB9vp/fig-1-8.png\" alt=\"fig7\" style=\"width:400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "* A acurácia final do modelo pode ser computada pegando média da acurácia do validação de dados do k-models. $$acc_{cv} = \\sum^{k}_{i=1} \\frac{acc_i}{k}$$\n",
    "\n",
    "* Ou seja, LOOCV é uma variante do k-fold cross-validation onde k = n.\n",
    "\n",
    "* Vantagens:\n",
    "    * 1 - O modelo tem viés pequeno;\n",
    "    * 2 - Complexidade de baixo tempo;\n",
    "    * 3 - Todo o conjunto de dados é usado no treinamento e na validação;\n",
    "* Desvantagens:\n",
    "    * 1 - Não é recomendade para um conjunto de dados desequilibrado.\n",
    "    \n",
    "### 3.3.3 Python    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1a73af-caec-40ba-b1e0-bae081203193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "#Importing required libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "#Loading the dataset\n",
    "data = load_breast_cancer(as_frame = True)\n",
    "df = data.frame\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    " \n",
    "#Implementing cross validation\n",
    " \n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09ec6c-b400-41ef-8571-dedfa1d612c6",
   "metadata": {},
   "source": [
    "## 3.4 - In-time validate, in-time test / in-time validate, out-of-time test\n",
    "\n",
    "### 3.4.1 - Opção 1: in-time validate:\n",
    "\n",
    "Vamos considerar o que acontece se ignorarmos a dimensão de tempo dos dados e dividir todos os dados rotulados, independentemente de sua dimensão de tempo. Treinamos, validamos e testamos o modelo usando dados do mesmo período. Fazemos previsões para novos dados (digamos, 2019) usando um modelo treinado, validado e testado em dados rotulados de 2015-2018. A figura abaixo ilustra essa configuração. A vantagem é que usamos os dados mais recentes para treinar o modelo. A desvantagem é que pressupõe que os relacionamentos que existiam no passado serão os mesmos no futuro. Se os relacionamentos mudarem ao longo do tempo, nossas medidas de desempenho estimadas (que calculamos usando dados de teste in-time) irão exagerar o verdadeiro desempenho do modelo, ou seja, uma vez implantado, o modelo terá um desempenho pior do que esperamos.\n",
    "\n",
    "<img src=\"https://i.ibb.co/bsP9kS9/fig-1-11.png\" alt=\"fig7\" style=\"width:500px;\"/>\n",
    "\n",
    "### 3.4.2 - Opção 2: in-time validate, out-of-time test\n",
    "\n",
    "Outra possibilidade é mantermos o ano de 2018, treinar o modelo usando dados de 2015–2017 e testá-lo nos dados de 2018. A vantagem dessa abordagem é que avaliamos explicitamente a capacidade do modelo de prever fora do tempo. A desvantagem é que a capacidade de prever o atraso não é levada em consideração ao construir o modelo. Isso ocorre porque selecionamos o modelo que tem um bom desempenho nos dados de validação in-time. Além disso, a menos que treinemos novamente o modelo usando todos os dados de 2015-2018, não estamos usando os dados mais recentes para fazer previsões.\n",
    "\n",
    "<img src = \"https://i.ibb.co/tPs7DZH/fig-1-10.png\" alt=\"fig8\" style=\"width:500px;\"/>\n",
    "\n",
    "### 3.4.3 - Extra: Walk Foward Validation\n",
    "\n",
    "Roger Stein sugere modelos de treinamento em janelas sucessivas (potencialmente deslizantes) de dados e fazendo previsões para o próximo período de tempo. Cada vez, treinamos novamente o modelo usando todas as observações em um determinado momento e fazemos previsões para o próximo período. As previsões de cada janela são combinadas em um conjunto de previsões. Fazemos isso para muitos modelos diferentes e escolhemos aquele que cria as melhores previsões combinadas. A vantagem dessa abordagem é que a construção do modelo (ou seja, testar quais modelos funcionam melhor) leva explicitamente em conta a capacidade do modelo de fazer previsões fora do prazo. Uma vez que o modelo é reajustado para cada janela, esta abordagem simula um retreinamento periódico do modelo que acontece (ou deveria acontecer) na prática. A desvantagem da abordagem é que sempre validamos as previsões no mesmo conjunto de dados, em vez de dados selecionados aleatoriamente. (Talvez exibir uma parte dos dados de teste possa superar essa preocupação.)\n",
    "\n",
    "<img src = \"https://i.ibb.co/6wmrygW/fig-1-12.png\" alt=\"fig9\" style=\"width:500px;\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
